{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea0c609d",
   "metadata": {},
   "source": [
    "## Example 1: extracting a paragraph from wikipedia page\n",
    "\n",
    "The following function converts Erik Durm's wikipedia page into a soup object and extracts the `<p>` element that starts with \"In the 2013–14 Bundesliga season...\". The function return the Beautiful Soup's `Tag` object itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "03326859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "1e488de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import sys\n",
    "import random\n",
    "import requests\n",
    "import scipy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import HTML, display, Image\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "0e0b005e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the 2013–14 Bundesliga season, Durm was inducted into Borussia Dortmund's first team and, on 10 August 2013, debuted for BVB in the Bundesliga; coming on in the 87th minute for Robert Lewandowski as a substitute in BVB's 4–0 win over FC Augsburg.[11][12] Durm debuted in the UEFA Champions League on 1 October 2013 in a 3–0 victory over French club Olympique Marseille.\n",
      "<class 'bs4.element.Tag'>\n"
     ]
    }
   ],
   "source": [
    "def extract_bundesliga_p_tag(wiki_path):\n",
    "        '''\n",
    "        Extracts the <p> tag from ErikDurmWiki that starts with:\n",
    "        \"In the 2013-14 Bundesliga season.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        wiki_path : (str)\n",
    "            Path to Erik Durm's wikipedia page in the Data folder.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        p_tag : (bs4.element.tag)   \n",
    "            Beautiful Soup's tag object containing the paragraph in question.\n",
    "        ''' \n",
    "        \n",
    "        p_tag  = None \n",
    "        # read html file with Beautiful Soup \n",
    "        with open(wiki_path, \"r\", encoding = \"utf-8\") as wiki_file:\n",
    "            string_content = wiki_file.read()\n",
    "            soup = BeautifulSoup(string_content,'lxml')\n",
    "        line = 'In the 2013–14 Bundesliga season' # the target line we are searching for \n",
    "        paragraphs = soup.find_all('p') # find all paragraphs \n",
    "        for paragraph in paragraphs:\n",
    "            if line in paragraph.text: # search paragraph with the target line \n",
    "                p_tag = paragraph        \n",
    "        return p_tag \n",
    "    \n",
    "    \n",
    "    # call the functio \n",
    "wiki_path = \"./Data/web_scraping_erik_durm_wiki.html\"\n",
    "p_tag = extract_bundesliga_p_tag(wiki_path)\n",
    "\n",
    "print(p_tag.text)\n",
    "print(type(p_tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc51ae3",
   "metadata": {},
   "source": [
    "## Example 2: extract song title and lyrics \n",
    "\n",
    "The following function converts the webpage ([Pink Floyd Lyrics]) containing the lyrics to Pink Floyd's Wish You Were Here into a soup and returns a list with the song's title and lyrics. The lyrics is formatted as a list of strings, with each string being a line in the lyrics. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "24e5d045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.Tag'>\n",
      "Wish You Were Here\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Wish You Were Here',\n",
       " ['So, so you think you can tell Heaven from Hell, blue skies from pain.',\n",
       "  'Can you tell a green field from a cold steel rail?',\n",
       "  'A smile from a veil?',\n",
       "  'Do you think you can tell?',\n",
       "  'Did they get you to trade your heroes for ghosts?',\n",
       "  'Hot ashes for trees?',\n",
       "  'Hot air for a cool breeze?',\n",
       "  'Cold comfort for change?',\n",
       "  'Did you exchange a walk on part in the war for a lead role in a cage?',\n",
       "  'How I wish, how I wish you were here.',\n",
       "  \"We're just two lost souls swimming in a fish bowl, year after year,\",\n",
       "  'Running over the same old ground.',\n",
       "  'What have we found?',\n",
       "  'The same old fears.',\n",
       "  'Wish you were here.']]"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_pink_floyd_lyrics(lyrics_path):\n",
    "    '''\n",
    "    Extracts the title and lyrics of Pink Floyd's Wish You Were Here song.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    lyrics_path : (str)\n",
    "        Path to the lyrics page in the Data folder.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    lyrics_info : (list)\n",
    "        List where the first element is the song's title (string)\n",
    "        and second element is a list of strings with the lyrics\n",
    "    '''\n",
    "    \n",
    "    lyrics_info  = []\n",
    "    # read html file and convert it to a soup object \n",
    "    with open(lyrics_path, \"r\", encoding = \"utf-8\") as wiki_file:\n",
    "        string_content = wiki_file.read()\n",
    "        soup = BeautifulSoup(string_content,'lxml')\n",
    "    \n",
    "    # find song name  \n",
    "    title = soup.find('title') \n",
    "    print(type(title))\n",
    "    song_name = title.text.split('-')[-1].strip()\n",
    "    print(song_name)\n",
    "    lyrics_info.append(song_name)\n",
    "    \n",
    "    \n",
    "    # find lyrics \n",
    "    div = soup.find('div',class_ = \"ringtone\")\n",
    "    lyrics_tag =div.find_next_sibling('div')\n",
    "    lyrics_str = lyrics_tag.text\n",
    "    lyrics_list = lyrics_str.strip().split('\\n')\n",
    "    while(\"\" in lyrics_list):\n",
    "        lyrics_list.remove(\"\")\n",
    "    lyrics_info.append(lyrics_list)\n",
    "    return lyrics_info\n",
    "\n",
    "lyrics_path =  './Data/pink_floyd_wish.html'\n",
    "extract_pink_floyd_lyrics(lyrics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b0d175",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
